<!doctype html>



  


<html class="theme-next pisces use-motion">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>



<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />












  
  
  <link href="/vendors/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/vendors/font-awesome/css/font-awesome.min.css?v=4.4.0" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.0.1" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="Hello, World" />








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.0.1" />






<meta property="og:type" content="website">
<meta property="og:title" content="JiananBear's Notes">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="JiananBear's Notes">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="JiananBear's Notes">



<script type="text/javascript" id="hexo.configuration">
  var NexT = window.NexT || {};
  var CONFIG = {
    scheme: 'Pisces',
    sidebar: {"position":"left","display":"post"},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: 0,
      author: '博主'
    }
  };
</script>




  <link rel="canonical" href="http://yoursite.com/"/>

  <title> JiananBear's Notes </title>
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  










  
  
    
  

  <div class="container one-collumn sidebar-position-left 
   page-home 
 ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-meta ">
  

  <div class="custom-logo-site-title">
    <a href="/"  class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <span class="site-title">JiananBear's Notes</span>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>
  <p class="site-subtitle">逆旅上的行人</p>
</div>

<div class="site-nav-toggle">
  <button>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
  </button>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup">
 <span class="search-icon fa fa-search"></span>
 <input type="text" id="local-search-input">
 <div id="local-search-result"></div>
 <span class="popup-btn-close">close</span>
</div>


    </div>
  
</nav>

 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2016/08/13/Spark-自定义数据源的探索/" itemprop="url">
                  Spark 自定义数据源的探索
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2016-08-13T10:44:20+08:00" content="2016-08-13">
              2016-08-13
            </time>
          </span>

          

          
            
              <span class="post-comments-count">
                &nbsp; | &nbsp;
                <a href="/2016/08/13/Spark-自定义数据源的探索/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count ds-thread-count" data-thread-key="2016/08/13/Spark-自定义数据源的探索/" itemprop="commentsCount"></span>
                </a>
              </span>
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="目的"><a href="#目的" class="headerlink" title="目的"></a>目的</h2><blockquote>
<ol>
<li>目的是实现一个基于HDFS之上自定义的外部数据源.</li>
<li>能够被spark加载转化成为DataFram，从而关联Spark SQL。</li>
<li>我们可以将DataFrame转化成自定义的数据格式，写入到HDFS文件中。</li>
</ol>
</blockquote>
<p>  我们自定义的数据源能像Spark内部实现的Text、Json数据源一样，可以通过以下两种方式被加载：</p>
<ol>
<li><p>通过DataFrameReader的方式</p>
 <figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sqlContext.read.format(<span class="string">"xxx"</span>).opinon(<span class="string">"a"</span>, <span class="string">"xxxx"</span>).load(<span class="string">"path"</span>)</div></pre></td></tr></table></figure>
</li>
<li><p>通过sql的方式</p>
 <figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">val ddl =</div><div class="line">     s"""</div><div class="line">        |CREATE TEMPORARY TABLE text</div><div class="line">        |USING "text/json/xxxx"</div><div class="line">        |OPTIONS (</div><div class="line">        | path  'xxxxxx',</div><div class="line">        |)""".stripMargin</div><div class="line">    sqlContext.sql(ddl)</div><div class="line">    sqlContext.sql("select * from text")</div></pre></td></tr></table></figure>
</li>
</ol>
<h2 id="自定义HDFS上的数据存储格式"><a href="#自定义HDFS上的数据存储格式" class="headerlink" title="自定义HDFS上的数据存储格式"></a>自定义HDFS上的数据存储格式</h2><p>Spark中读取HDFS上的数据是通过HadoopRDD来实现的。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">HadoopRDD</span>[<span class="type">K</span>, <span class="type">V</span>](<span class="params"></span></span></div><div class="line">    sc: <span class="type">SparkContext</span>,</div><div class="line">    broadcastedConf: <span class="type">Broadcast</span>[<span class="type">SerializableConfiguration</span>],</div><div class="line">    initLocalJobConfFuncOpt: <span class="type">Option</span>[<span class="type">JobConf</span> =&gt; <span class="type">Unit</span>],</div><div class="line">    inputFormatClass: <span class="type">Class</span>[_ &lt;: <span class="type">InputFormat</span>[<span class="type">K</span>, <span class="type">V</span>]],</div><div class="line">    keyClass: <span class="type">Class</span>[<span class="type">K</span>],</div><div class="line">    valueClass: <span class="type">Class</span>[<span class="type">V</span>],</div><div class="line">    minPartitions: <span class="type">Int</span>)</div><div class="line">  <span class="keyword">extends</span> <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">V</span>)](sc, <span class="type">Nil</span>)</div></pre></td></tr></table></figure>
<p>从HadoopRDD的构造函数中，我们可以发现数据加载策略依赖于Hadoop的inputFormat，因此想要加载自定义的数据存储格式，就需要定制化一个InputFormat。<br>关于InputFormat包含两个关键方法</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">public interface <span class="type">InputFormat</span>&lt;<span class="type">K</span>, <span class="type">V</span>&gt; &#123;</div><div class="line"></div><div class="line">  <span class="type">InputSplit</span>[] getSplits(<span class="type">JobConf</span> job, int numSplits) <span class="keyword">throws</span> <span class="type">IOException</span>;</div><div class="line"></div><div class="line">  <span class="type">RecordReader</span>&lt;<span class="type">K</span>, <span class="type">V</span>&gt; getRecordReader(<span class="type">InputSplit</span> split,</div><div class="line">                                     <span class="type">JobConf</span> job, </div><div class="line">                                     <span class="type">Reporter</span> reporter) <span class="keyword">throws</span> <span class="type">IOException</span>;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="getSplits函数"><a href="#getSplits函数" class="headerlink" title="getSplits函数"></a>getSplits函数</h3><p>用于分割inputFile成多个Split，Split的数量在Hadoop控制中控制着加载数据的Mapper的数量，在Spark中也同样影响加载数据的Task的数量。</p>
<p>Spilt定义数据分片的相关信息，我觉得Split包含两个大的功能：</p>
<ol>
<li>Split包含了该分片数据Location相关的信息，换句话说就是Split数据在集群中的那些节点上，这个信息有助于Spark将加载数据的任务下发到Split数据所在的节点上。</li>
<li>Split还包含了<code>getLength</code>方法，主要就是获取Split的数据长度。在Executer处理的时候知道什么时候结束加载任务。</li>
</ol>
<h3 id="getRecordReader函数"><a href="#getRecordReader函数" class="headerlink" title="getRecordReader函数"></a>getRecordReader函数</h3><p>这个函数返回一个RecordReader，这个RecordReader类顾名思义，就是确定如何加载文件中的Record。<br>RecordReader 中最为关键的就是next方法，在这个方法中，我们定义逐个加载Record的策略，每个Record必须是key-value的形式。<br><code>next</code>函数返回<code>true</code>，那么这个Record有效并且会继续加载数据解析下一个Record，如果返回<code>false</code>表示这个split中的数据加载完毕，这个分片中的数据将不再被解析。</p>
<p>Hadoop根据不同的需求提供了不同类型的Split、RecordReader、IntputFormat，可以根据不同需求选取对应的类来实现自定义加载数据的需求。</p>
<h2 id="Spark-Extra-DataSource-的实现"><a href="#Spark-Extra-DataSource-的实现" class="headerlink" title="Spark Extra DataSource 的实现"></a>Spark Extra DataSource 的实现</h2><p>接下来就是让Spark SQL支持我们自定义的数据源。在Spark中是通过Relation来定义数据的schema以及扫描数据的方式。</p>
<p>实现一个Relation 需要具备三个条件</p>
<blockquote>
<ol>
<li>必须是BaseRelation的子类</li>
<li>需要实现某个Scan特质</li>
<li>定义一个equality function </li>
</ol>
</blockquote>
<p>BaseRelation 描述一个带有schema的元组，最主要的就是schema属性，定义生成的PhysicalRDD的schema。</p>
<p>Scan特质分为TableScan， PrunedScan，PrunedFilterScan，CatalystScan。</p>
<ol>
<li>TableScan：默认的Scan策略。</li>
<li>PrunedScan：这里可以传入指定的列，requiredColumns，列裁剪，不需要的列不会从外部数据源加载。</li>
<li>PrunedFilterScan：在列裁剪的基础上，并且加入Filter机制，在加载数据也的时候就进行过滤，而不是在客户端请求返回时做Filter。</li>
<li>CatalystScan：Catalyst的支持传入expressions来进行Scan。支持列裁剪和Filter。</li>
</ol>
<p>equality function主要的作用确定什么时候替换cached result的</p>
<h3 id="简单的例子"><a href="#简单的例子" class="headerlink" title="简单的例子"></a>简单的例子</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">TextRelation</span>(<span class="params">parameters: <span class="type">Map</span>[<span class="type">String</span>, <span class="type">String</span>] = <span class="type">Map</span>.empty[<span class="type">String</span>, <span class="type">String</span>]</span>)</span></div><div class="line">                  (<span class="meta">@transient</span> <span class="keyword">val</span> sqlContext: <span class="type">SQLContext</span>) <span class="keyword">extends</span> <span class="type">BaseRelation</span> <span class="keyword">with</span> <span class="type">TableScan</span> &#123;</div><div class="line"></div><div class="line">  <span class="keyword">val</span> path = parameters.getOrElse(<span class="string">"path"</span>, sys.error(<span class="string">"not valid schema"</span>)) <span class="comment">// parameters是个Map，传入什么可以自己定义</span></div><div class="line">  </div><div class="line">  <span class="keyword">override</span> <span class="keyword">val</span> schema: <span class="type">StructType</span> = &#123;</div><div class="line">  	<span class="comment">// 定义schema，方式有很多</span></div><div class="line">  	<span class="comment">// 可以从数据库中取</span></div><div class="line">  	<span class="comment">// 可以从文件中读</span></div><div class="line">  	<span class="comment">// 等等</span></div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">buildScan</span></span>(): <span class="type">RDD</span>[<span class="type">Row</span>] = &#123;</div><div class="line"> 	<span class="comment">// 可以根据parameters中的参数定制加载数据的策略</span></div><div class="line"> 	<span class="comment">// 如果加载hdfs上的数据，主要就是通过HadoopRDD从HDFS上将数据加载进来，然后转为RDD[Row]的形式。</span></div><div class="line"> 	<span class="comment">// 还可以加载其他数据库的数据，只要转后转化为RDD[Row]的形式就可以了</span></div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="comment">// 实现equal函数</span></div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>有了一个简单Relation，还需要提供一个Provider。<br>在每次调用执行DDL语句的时候，Provider实例会被创建，用于构建我们定义的Relation，这样才能支持开篇目标中提到的sql创建的方式</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">trait</span> <span class="title">RelationProvider</span> </span>&#123;</div><div class="line">  <span class="comment">/**</span></div><div class="line">   * Returns a new base relation with the given parameters.</div><div class="line">   */</div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">createRelation</span></span>(sqlContext: <span class="type">SQLContext</span>, parameters: <span class="type">Map</span>[<span class="type">String</span>, <span class="type">String</span>]): <span class="type">BaseRelation</span></div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h2 id="关于数据的写入"><a href="#关于数据的写入" class="headerlink" title="关于数据的写入"></a>关于数据的写入</h2><p>目前获取的思路就是将<code>RDD[Row]</code>转化为<code>RDD[(k,v)]</code>的形式，然后调用<code>SaveAsHadoopFile</code></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">saveAsHadoopFile</span></span>(</div><div class="line">      path: <span class="type">String</span>,</div><div class="line">      keyClass: <span class="type">Class</span>[_],</div><div class="line">      valueClass: <span class="type">Class</span>[_],</div><div class="line">      outputFormatClass: <span class="type">Class</span>[_ &lt;: <span class="type">OutputFormat</span>[_, _]],</div><div class="line">      codec: <span class="type">Class</span>[_ &lt;: <span class="type">CompressionCodec</span>])</div></pre></td></tr></table></figure>
<p>我们需要实现自定义一个outputFormat来实现数据的存储。关于outputFormat我们还需要定义三个函数</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line">public <span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">OutputFormat&lt;K</span>, <span class="title">V&gt;</span> </span>&#123;</div><div class="line"></div><div class="line">  <span class="comment">// RecordWriter 的主要作用就是负责Record如何写入hdfs文件中</span></div><div class="line">  <span class="comment">// 我们可以从context函数中获取输出的文件路径，通过调用FileSystem来创建输出流，然后将输出流传给RecordWriter</span></div><div class="line">  public <span class="keyword">abstract</span> <span class="type">RecordWriter</span>&lt;<span class="type">K</span>, <span class="type">V</span>&gt; </div><div class="line">    getRecordWriter(<span class="type">TaskAttemptContext</span> context</div><div class="line">                    ) <span class="keyword">throws</span> <span class="type">IOException</span>, <span class="type">InterruptedException</span>;</div><div class="line"></div><div class="line">  <span class="comment">// 判断一下是否满足执行job的条件</span></div><div class="line">  <span class="comment">// 可以判断输出的文件是否已经存在</span></div><div class="line">  <span class="comment">// 会在JobClient提交Job之前被调用的（在使用InputFomat进行输入数据划分之前），用于检测Job的输出路径</span></div><div class="line">  public <span class="keyword">abstract</span> void checkOutputSpecs(<span class="type">JobContext</span> context</div><div class="line">                                        ) <span class="keyword">throws</span> <span class="type">IOException</span>, </div><div class="line">                                                 <span class="type">InterruptedException</span>;</div><div class="line">  <span class="comment">// OutputCommitter用于控制Job的输出环境</span></div><div class="line">  <span class="comment">// OutputCommitter 包含 setupTask、commitTask 等函数</span></div><div class="line">  <span class="comment">//主要的作用就是在 task 执行之前会创建task对应的临时目录，当task执行完成之后，会将临时目录下面的文件move到最终的输出目录</span></div><div class="line">  <span class="comment">// 同时 OutputCommitter 还包含 setupJob、commitJob 等函数主要的作用也是在 Job 任务执行之前创建job对应的零食目录，在job执行完成之后将数据move到最终的目录。</span></div><div class="line">  public <span class="keyword">abstract</span> <span class="type">OutputCommitter</span> getOutputCommitter(<span class="type">TaskAttemptContext</span> context</div><div class="line">                                     ) <span class="keyword">throws</span> <span class="type">IOException</span>, <span class="type">InterruptedException</span>;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>更多有关OutputFormat更加详细的信息，见参考资料<code>Hadoop OutputFormat浅析</code></p>
<p>实现了OutPutFormat，我们只需要将DataFrame转化为RDD[(key, value)]的形式，就能调用<code>saveAsHadoopFile</code>实现存储。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">df.rdd.map(row =&gt;</div><div class="line">  (key, value)</div><div class="line">).saveAsNewApiHadoopFile(&quot;path&quot;, classOf[Key], classOf[Value], classOf[CusOutputFormat])</div></pre></td></tr></table></figure>
<h2 id="自定义数据源的改进"><a href="#自定义数据源的改进" class="headerlink" title="自定义数据源的改进"></a>自定义数据源的改进</h2><p>以上的写入方式依旧是不够优雅，那么如何才能实现</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">df.writer.format(&quot;xxxx&quot;).mode(&quot;xxx&quot;).save(&quot;path&quot;)</div></pre></td></tr></table></figure>
<p>跟进一下DataFrameWriter的<code>save</code>函数的代码发现其中创建了一个<code>ResolvedDataSource</code>实例，在创建的过程中会根据<code>format</code>确定<code>provider</code>的信息，并通过反射得到相应的<code>Relation</code>，DataFrameWriter目前支持的<code>Relation</code>只能是<code>CreatableRelation</code>和<code>HadoopFsRelation</code>的子类。</p>
<p>因此，修改自定义的<code>Relation</code>，使其继承<code>HadoopFsRelation</code>。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// TextRelation 构造参数可以随意</span></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">TextRelation</span>(<span class="params">override val paths: <span class="type">Array</span>[<span class="type">String</span>], parameters: <span class="type">Map</span>[<span class="type">String</span>, <span class="type">String</span>] = <span class="type">Map</span>.empty[<span class="type">String</span>, <span class="type">String</span>]</span>)(<span class="params">@transient val sqlContext: <span class="type">SQLContext</span></span>) <span class="keyword">extends</span> <span class="title">HadoopFsRelation</span> </span>&#123;</div><div class="line">  <span class="comment">// 不在需要重载schema，只需要重载dataSchema，在HadoopFsRelation内部会使用dataSchema来构造schema</span></div><div class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">dataSchema</span></span>: <span class="type">StructType</span> = &#123;</div><div class="line">    ...</div><div class="line">  &#125;</div><div class="line"> <span class="comment">// HadoopFsRelation 需要提供另一个Writer的工厂类</span></div><div class="line"> <span class="comment">// 这是和之前的例子有区别的地方，返回的OutputWriter用于数据的写入</span></div><div class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">prepareJobForWrite</span></span>(job: <span class="type">Job</span>): <span class="type">OutputWriterFactory</span> = &#123;</div><div class="line">    <span class="keyword">new</span> <span class="type">OutputWriterFactory</span> &#123;</div><div class="line">      <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">newInstance</span></span>(</div><div class="line">                                path: <span class="type">String</span>,</div><div class="line">                                dataSchema: <span class="type">StructType</span>,</div><div class="line">                                context: <span class="type">TaskAttemptContext</span>): <span class="type">OutputWriter</span> = &#123;</div><div class="line">        <span class="keyword">new</span> <span class="type">XXXOutputWriter</span>(path, dataSchema, context)</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line"><span class="comment">// 加载数据生成RDD[Row]，这里举了一个简单的例子</span></div><div class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">buildScan</span></span>(inputFiles: <span class="type">Array</span>[<span class="type">FileStatus</span>]): <span class="type">RDD</span>[<span class="type">Row</span>] = &#123;</div><div class="line">    <span class="keyword">val</span> job = <span class="keyword">new</span> <span class="type">Job</span>(sqlContext.sparkContext.hadoopConfiguration)</div><div class="line">    <span class="keyword">val</span> conf = <span class="type">SparkHadoopUtil</span>.get.getConfigurationFromJobContext(job)</div><div class="line"></div><div class="line">    <span class="keyword">if</span> (paths.nonEmpty) &#123;</div><div class="line">      <span class="comment">// 设置InputFormat路径</span></div><div class="line">      <span class="type">FileInputFormat</span>.setInputPaths(job, paths.map(<span class="keyword">new</span> <span class="type">Path</span>(_)): _*)</div><div class="line">    &#125;</div><div class="line">    <span class="keyword">val</span> rows = sqlContext.sparkContext.hadoopRDD(</div><div class="line">      conf.asInstanceOf[<span class="type">JobConf</span>], classOf[<span class="type">CusTextInputFormat</span>], classOf[<span class="type">LongWritable</span>], classOf[<span class="type">Text</span>])</div><div class="line">      <span class="comment">// 根据我们自定义的CusTextInputFormat，这个rows变量的类型是RDD[(LongWritable, Text)]</span></div><div class="line">      <span class="comment">// 我们需要转化为RDD[Row]的形式返回</span></div><div class="line">    rows.map[<span class="type">Row</span>] &#123; pair =&gt;</div><div class="line">      <span class="keyword">val</span> values = pair._2.toString.split(<span class="string">","</span>)</div><div class="line">      <span class="type">Row</span>(values: _*)</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>我们来看看<code>OutputWriter</code>应该如何实现，</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// 这部分代码自然是在excuter端运行的</span></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">TextOutputWriter</span>(<span class="params">path: <span class="type">String</span>, dataSchema: <span class="type">StructType</span>, context: <span class="type">TaskAttemptContext</span></span>) <span class="keyword">extends</span> <span class="title">OutputWriter</span> </span>&#123;</div><div class="line"></div><div class="line">  <span class="keyword">private</span> <span class="keyword">val</span> recordWriter: <span class="type">RecordWriter</span>[<span class="type">NullWritable</span>, <span class="type">Row</span>] = &#123;</div><div class="line">    <span class="comment">// 创建我们自定义的OutputFormat</span></div><div class="line">    <span class="keyword">new</span> <span class="type">CusTextOutputFormat</span>() &#123;</div><div class="line">    &#125;.getRecordWriter(context)</div><div class="line">  &#125;</div><div class="line">    </div><div class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">write</span></span>(row: <span class="type">Row</span>): <span class="type">Unit</span> = &#123;</div><div class="line">    recordWriter.write(<span class="type">NullWritable</span>.get(), row)</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">close</span></span>(): <span class="type">Unit</span> = &#123;</div><div class="line">    recordWriter.close(context)</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>最后提供一个<code>HadoopFsRelationProvider</code>就能使用我们自己定义的数据源了。</p>
<h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><ol>
<li>如何让<code>create table xxx stored as &#39;自定义的格式&#39; .....</code> 创建语句有效，还需要调研。</li>
<li>如何实现类似Parquet格式的数据写入，还需要研究。</li>
</ol>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol>
<li><a href="https://yq.aliyun.com/articles/8877" target="_blank" rel="external">Hadoop OutputFormat浅析</a></li>
<li><a href="http://my.oschina.net/leejun2005/blog/133424" target="_blank" rel="external">自定义 hadoop MapReduce InputFormat 切分输入文件</a></li>
<li><a href="https://github.com/databricks/spark-csv" target="_blank" rel="external">databricks/spark-csv</a></li>
<li><a href="https://github.com/JiananBear/Extra-DataSource-Demo/tree/master" target="_blank" rel="external">我自己写了一个小demo</a></li>
</ol>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
  </section>

  


          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      

      <section class="site-overview sidebar-panel  sidebar-panel-active ">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/images/avatar.PNG"
               alt="Jianan Xiong" />
          <p class="site-author-name" itemprop="name">Jianan Xiong</p>
          <p class="site-description motion-element" itemprop="description"></p>
        </div>
        <nav class="site-state motion-element">
          <div class="site-state-item site-state-posts">
            <a href="/archives">
              <span class="site-state-item-count">1</span>
              <span class="site-state-item-name">日志</span>
            </a>
          </div>

          

          
            <div class="site-state-item site-state-tags">
              <a href="/tags">
                <span class="site-state-item-count">1</span>
                <span class="site-state-item-name">标签</span>
              </a>
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
        </div>

        
        

        
        

      </section>

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2016</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Jianan Xiong</span>
</div>

<div class="powered-by">
  由 <a class="theme-link" href="https://hexo.io">Hexo</a> 强力驱动
</div>

<div class="theme-info">
  主题 -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Pisces
  </a>
</div>

        

        
      </div>
    </footer>

    <div class="back-to-top">
      <i class="fa fa-arrow-up"></i>
    </div>
  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  



  
  <script type="text/javascript" src="/vendors/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/vendors/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/vendors/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/vendors/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/vendors/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/vendors/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.0.1"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.0.1"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.0.1"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.0.1"></script>



  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.0.1"></script>



  

  
    
  

  <script type="text/javascript">
    var duoshuoQuery = {short_name:"luckybear"};
    (function() {
      var ds = document.createElement('script');
      ds.type = 'text/javascript';ds.async = true;
      ds.id = 'duoshuo-script';
      ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
      ds.charset = 'UTF-8';
      (document.getElementsByTagName('head')[0]
      || document.getElementsByTagName('body')[0]).appendChild(ds);
    })();
  </script>

  
    
    <script src="/vendors/ua-parser-js/dist/ua-parser.min.js?v=0.7.9"></script>
    <script src="/js/src/hook-duoshuo.js"></script>
  






  
  
  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length == 0) {
       search_path = "search.xml";
    }
    var path = "/" + search_path;
    // monitor main search box;

    function proceedsearch() {
      $("body").append('<div class="popoverlay">').css('overflow', 'hidden');
      $('.popup').toggle();

    }
    // search function;
    var searchFunc = function(path, search_id, content_id) {
    'use strict';
    $.ajax({
        url: path,
        dataType: "xml",
        async: true,
        success: function( xmlResponse ) {
            // get the contents from search data
            isfetched = true;
            $('.popup').detach().appendTo('.header-inner');
            var datas = $( "entry", xmlResponse ).map(function() {
                return {
                    title: $( "title", this ).text(),
                    content: $("content",this).text(),
                    url: $( "url" , this).text()
                };
            }).get();
            var $input = document.getElementById(search_id);
            var $resultContent = document.getElementById(content_id);
            $input.addEventListener('input', function(){
                var matchcounts = 0;
                var str='<ul class=\"search-result-list\">';
                var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                $resultContent.innerHTML = "";
                if (this.value.trim().length > 1) {
                // perform local searching
                datas.forEach(function(data) {
                    var isMatch = true;
                    var content_index = [];
                    var data_title = data.title.trim().toLowerCase();
                    var data_content = data.content.trim().replace(/<[^>]+>/g,"").toLowerCase();
                    var data_url = data.url;
                    var index_title = -1;
                    var index_content = -1;
                    var first_occur = -1;
                    // only match artiles with not empty titles and contents
                    if(data_title != '' && data_content != '') {
                        keywords.forEach(function(keyword, i) {
                            index_title = data_title.indexOf(keyword);
                            index_content = data_content.indexOf(keyword);
                            if( index_title < 0 && index_content < 0 ){
                                isMatch = false;
                            } else {
                                if (index_content < 0) {
                                    index_content = 0;
                                }
                                if (i == 0) {
                                    first_occur = index_content;
                                }
                            }
                        });
                    }
                    // show search results
                    if (isMatch) {
                        matchcounts += 1;
                        str += "<li><a href='"+ data_url +"' class='search-result-title'>"+ data_title +"</a>";
                        var content = data.content.trim().replace(/<[^>]+>/g,"");
                        if (first_occur >= 0) {
                            // cut out 100 characters
                            var start = first_occur - 20;
                            var end = first_occur + 80;
                            if(start < 0){
                                start = 0;
                            }
                            if(start == 0){
                                end = 50;
                            }
                            if(end > content.length){
                                end = content.length;
                            }
                            var match_content = content.substring(start, end);
                            // highlight all keywords
                            keywords.forEach(function(keyword){
                                var regS = new RegExp(keyword, "gi");
                                match_content = match_content.replace(regS, "<b class=\"search-keyword\">"+keyword+"</b>");
                            });

                            str += "<p class=\"search-result\">" + match_content +"...</p>"
                        }
                        str += "</li>";
                    }
                })};
                str += "</ul>";
                if (matchcounts == 0) { str = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>' }
                if (keywords == "") { str = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>' }
                $resultContent.innerHTML = str;
            });
            proceedsearch();
        }
    });}

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched == false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };

    });

    $('.popup-btn-close').click(function(e){
      $('.popup').hide();
      $(".popoverlay").remove();
      $('body').css('overflow', '');
    });
    $('.popup').click(function(e){
      e.stopPropagation();
    });
  </script>


  

  

  

</body>
</html>
