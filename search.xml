<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
    
    <entry>
      <title><![CDATA[Spark 自定义数据源的探索]]></title>
      <url>http://yoursite.com/2016/08/13/Spark-%E8%87%AA%E5%AE%9A%E4%B9%89%E6%95%B0%E6%8D%AE%E6%BA%90%E7%9A%84%E6%8E%A2%E7%B4%A2/</url>
      <content type="html"><![CDATA[<h2 id="目的"><a href="#目的" class="headerlink" title="目的"></a>目的</h2><blockquote>
<ol>
<li>目的是实现一个基于HDFS之上自定义的外部数据源.</li>
<li>能够被spark加载转化成为DataFram，从而关联Spark SQL。</li>
<li>我们可以将DataFrame转化成自定义的数据格式，写入到HDFS文件中。</li>
</ol>
</blockquote>
<p>  我们自定义的数据源能像Spark内部实现的Text、Json数据源一样，可以通过以下两种方式被加载：</p>
<ol>
<li><p>通过DataFrameReader的方式</p>
 <figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sqlContext.read.format(<span class="string">"xxx"</span>).opinon(<span class="string">"a"</span>, <span class="string">"xxxx"</span>).load(<span class="string">"path"</span>)</div></pre></td></tr></table></figure>
</li>
<li><p>通过sql的方式</p>
 <figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">val ddl =</div><div class="line">     s"""</div><div class="line">        |CREATE TEMPORARY TABLE text</div><div class="line">        |USING "text/json/xxxx"</div><div class="line">        |OPTIONS (</div><div class="line">        | path  'xxxxxx',</div><div class="line">        |)""".stripMargin</div><div class="line">    sqlContext.sql(ddl)</div><div class="line">    sqlContext.sql("select * from text")</div></pre></td></tr></table></figure>
</li>
</ol>
<h2 id="自定义HDFS上的数据存储格式"><a href="#自定义HDFS上的数据存储格式" class="headerlink" title="自定义HDFS上的数据存储格式"></a>自定义HDFS上的数据存储格式</h2><p>Spark中读取HDFS上的数据是通过HadoopRDD来实现的。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">HadoopRDD</span>[<span class="type">K</span>, <span class="type">V</span>](<span class="params"></span></span></div><div class="line">    sc: <span class="type">SparkContext</span>,</div><div class="line">    broadcastedConf: <span class="type">Broadcast</span>[<span class="type">SerializableConfiguration</span>],</div><div class="line">    initLocalJobConfFuncOpt: <span class="type">Option</span>[<span class="type">JobConf</span> =&gt; <span class="type">Unit</span>],</div><div class="line">    inputFormatClass: <span class="type">Class</span>[_ &lt;: <span class="type">InputFormat</span>[<span class="type">K</span>, <span class="type">V</span>]],</div><div class="line">    keyClass: <span class="type">Class</span>[<span class="type">K</span>],</div><div class="line">    valueClass: <span class="type">Class</span>[<span class="type">V</span>],</div><div class="line">    minPartitions: <span class="type">Int</span>)</div><div class="line">  <span class="keyword">extends</span> <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">V</span>)](sc, <span class="type">Nil</span>)</div></pre></td></tr></table></figure>
<p>从HadoopRDD的构造函数中，我们可以发现数据加载策略依赖于Hadoop的inputFormat，因此想要加载自定义的数据存储格式，就需要定制化一个InputFormat。<br>关于InputFormat包含两个关键方法</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">public interface <span class="type">InputFormat</span>&lt;<span class="type">K</span>, <span class="type">V</span>&gt; &#123;</div><div class="line"></div><div class="line">  <span class="type">InputSplit</span>[] getSplits(<span class="type">JobConf</span> job, int numSplits) <span class="keyword">throws</span> <span class="type">IOException</span>;</div><div class="line"></div><div class="line">  <span class="type">RecordReader</span>&lt;<span class="type">K</span>, <span class="type">V</span>&gt; getRecordReader(<span class="type">InputSplit</span> split,</div><div class="line">                                     <span class="type">JobConf</span> job, </div><div class="line">                                     <span class="type">Reporter</span> reporter) <span class="keyword">throws</span> <span class="type">IOException</span>;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="getSplits函数"><a href="#getSplits函数" class="headerlink" title="getSplits函数"></a>getSplits函数</h3><p>用于分割inputFile成多个Split，Split的数量在Hadoop控制中控制着加载数据的Mapper的数量，在Spark中也同样影响加载数据的Task的数量。</p>
<p>Spilt定义数据分片的相关信息，我觉得Split包含两个大的功能：</p>
<ol>
<li>Split包含了该分片数据Location相关的信息，换句话说就是Split数据在集群中的那些节点上，这个信息有助于Spark将加载数据的任务下发到Split数据所在的节点上。</li>
<li>Split还包含了<code>getLength</code>方法，主要就是获取Split的数据长度。在Executer处理的时候知道什么时候结束加载任务。</li>
</ol>
<h3 id="getRecordReader函数"><a href="#getRecordReader函数" class="headerlink" title="getRecordReader函数"></a>getRecordReader函数</h3><p>这个函数返回一个RecordReader，这个RecordReader类顾名思义，就是确定如何加载文件中的Record。<br>RecordReader 中最为关键的就是next方法，在这个方法中，我们定义逐个加载Record的策略，每个Record必须是key-value的形式。<br><code>next</code>函数返回<code>true</code>，那么这个Record有效并且会继续加载数据解析下一个Record，如果返回<code>false</code>表示这个split中的数据加载完毕，这个分片中的数据将不再被解析。</p>
<p>Hadoop根据不同的需求提供了不同类型的Split、RecordReader、IntputFormat，可以根据不同需求选取对应的类来实现自定义加载数据的需求。</p>
<h2 id="Extral-DataSource-的实现"><a href="#Extral-DataSource-的实现" class="headerlink" title="Extral DataSource 的实现"></a>Extral DataSource 的实现</h2><p>接下来就是让Spark SQL支持我们自定义的数据源。在Spark中是通过Relation来定义数据的schema以及扫描数据的方式。</p>
<p>实现一个Relation 需要具备三个条件</p>
<blockquote>
<ol>
<li>必须是BaseRelation的子类</li>
<li>需要实现某个Scan特质</li>
<li>定义一个equality function </li>
</ol>
</blockquote>
<p>BaseRelation 描述一个带有schema的元组，最主要的就是schema属性，定义生成的PhysicalRDD的schema。</p>
<p>Scan特质分为TableScan， PrunedScan，PrunedFilterScan，CatalystScan。</p>
<ol>
<li>TableScan：默认的Scan策略。</li>
<li>PrunedScan：这里可以传入指定的列，requiredColumns，列裁剪，不需要的列不会从外部数据源加载。</li>
<li>PrunedFilterScan：在列裁剪的基础上，并且加入Filter机制，在加载数据也的时候就进行过滤，而不是在客户端请求返回时做Filter。</li>
<li>CatalystScan：Catalyst的支持传入expressions来进行Scan。支持列裁剪和Filter。</li>
</ol>
<p>equality function主要的作用确定什么时候替换cached result的</p>
<h3 id="简单的例子"><a href="#简单的例子" class="headerlink" title="简单的例子"></a>简单的例子</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">TextRelation</span>(<span class="params">parameters: <span class="type">Map</span>[<span class="type">String</span>, <span class="type">String</span>] = <span class="type">Map</span>.empty[<span class="type">String</span>, <span class="type">String</span>]</span>)</span></div><div class="line">                  (<span class="meta">@transient</span> <span class="keyword">val</span> sqlContext: <span class="type">SQLContext</span>) <span class="keyword">extends</span> <span class="type">BaseRelation</span> <span class="keyword">with</span> <span class="type">TableScan</span> &#123;</div><div class="line"></div><div class="line">  <span class="keyword">val</span> path = parameters.getOrElse(<span class="string">"path"</span>, sys.error(<span class="string">"not valid schema"</span>)) <span class="comment">// parameters是个Map，传入什么可以自己定义</span></div><div class="line">  </div><div class="line">  <span class="keyword">override</span> <span class="keyword">val</span> schema: <span class="type">StructType</span> = &#123;</div><div class="line">  	<span class="comment">// 定义schema，方式有很多</span></div><div class="line">  	<span class="comment">// 可以从数据库中取</span></div><div class="line">  	<span class="comment">// 可以从文件中读</span></div><div class="line">  	<span class="comment">// 等等</span></div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">buildScan</span></span>(): <span class="type">RDD</span>[<span class="type">Row</span>] = &#123;</div><div class="line"> 	<span class="comment">// 可以根据parameters中的参数定制加载数据的策略</span></div><div class="line"> 	<span class="comment">// 如果加载hdfs上的数据，主要就是通过HadoopRDD从HDFS上将数据加载进来，然后转为RDD[Row]的形式。</span></div><div class="line"> 	<span class="comment">// 还可以加载其他数据库的数据，只要转后转化为RDD[Row]的形式就可以了</span></div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="comment">// 实现equal函数</span></div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>有了一个简单Relation，还需要提供一个Provider。<br>在每次调用执行DDL语句的时候，Provider实例会被创建，用于构建我们定义的Relation，这样才能支持开篇目标中提到的sql创建的方式</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">trait</span> <span class="title">RelationProvider</span> </span>&#123;</div><div class="line">  <span class="comment">/**</span></div><div class="line">   * Returns a new base relation with the given parameters.</div><div class="line">   */</div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">createRelation</span></span>(sqlContext: <span class="type">SQLContext</span>, parameters: <span class="type">Map</span>[<span class="type">String</span>, <span class="type">String</span>]): <span class="type">BaseRelation</span></div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h2 id="关于数据的写入"><a href="#关于数据的写入" class="headerlink" title="关于数据的写入"></a>关于数据的写入</h2><p>目前获取的思路就是将<code>RDD[Row]</code>转化为<code>RDD[(k,v)]</code>的形式，然后调用<code>SaveAsHadoopFile</code></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">saveAsHadoopFile</span></span>(</div><div class="line">      path: <span class="type">String</span>,</div><div class="line">      keyClass: <span class="type">Class</span>[_],</div><div class="line">      valueClass: <span class="type">Class</span>[_],</div><div class="line">      outputFormatClass: <span class="type">Class</span>[_ &lt;: <span class="type">OutputFormat</span>[_, _]],</div><div class="line">      codec: <span class="type">Class</span>[_ &lt;: <span class="type">CompressionCodec</span>])</div></pre></td></tr></table></figure>
<p>我们需要实现自定义一个outputFormat来实现数据的存储。关于outputFormat我们还需要定义三个函数</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line">public <span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">OutputFormat&lt;K</span>, <span class="title">V&gt;</span> </span>&#123;</div><div class="line"></div><div class="line">  <span class="comment">// RecordWriter 的主要作用就是负责Record如何写入hdfs文件中</span></div><div class="line">  <span class="comment">// 我们可以从context函数中获取输出的文件路径，通过调用FileSystem来创建输出流，然后将输出流传给RecordWriter</span></div><div class="line">  public <span class="keyword">abstract</span> <span class="type">RecordWriter</span>&lt;<span class="type">K</span>, <span class="type">V</span>&gt; </div><div class="line">    getRecordWriter(<span class="type">TaskAttemptContext</span> context</div><div class="line">                    ) <span class="keyword">throws</span> <span class="type">IOException</span>, <span class="type">InterruptedException</span>;</div><div class="line"></div><div class="line">  <span class="comment">// 判断一下是否满足执行job的条件</span></div><div class="line">  <span class="comment">// 可以判断输出的文件是否已经存在</span></div><div class="line">  <span class="comment">// 会在JobClient提交Job之前被调用的（在使用InputFomat进行输入数据划分之前），用于检测Job的输出路径</span></div><div class="line">  public <span class="keyword">abstract</span> void checkOutputSpecs(<span class="type">JobContext</span> context</div><div class="line">                                        ) <span class="keyword">throws</span> <span class="type">IOException</span>, </div><div class="line">                                                 <span class="type">InterruptedException</span>;</div><div class="line">  <span class="comment">// OutputCommitter用于控制Job的输出环境</span></div><div class="line">  <span class="comment">// OutputCommitter 包含 setupTask、commitTask 等函数</span></div><div class="line">  <span class="comment">//主要的作用就是在 task 执行之前会创建task对应的临时目录，当task执行完成之后，会将临时目录下面的文件move到最终的输出目录</span></div><div class="line">  <span class="comment">// 同时 OutputCommitter 还包含 setupJob、commitJob 等函数主要的作用也是在 Job 任务执行之前创建job对应的零食目录，在job执行完成之后将数据move到最终的目录。</span></div><div class="line">  public <span class="keyword">abstract</span> <span class="type">OutputCommitter</span> getOutputCommitter(<span class="type">TaskAttemptContext</span> context</div><div class="line">                                     ) <span class="keyword">throws</span> <span class="type">IOException</span>, <span class="type">InterruptedException</span>;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>更多有关OutputFormat更加详细的信息，见参考资料<code>Hadoop OutputFormat浅析</code></p>
<p>实现了OutPutFormat，我们只需要将DataFrame转化为RDD[(key, value)]的形式，就能调用<code>saveAsHadoopFile</code>实现存储。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">df.rdd.map(row =&gt;</div><div class="line">  (key, value)</div><div class="line">).saveAsNewApiHadoopFile(&quot;path&quot;, classOf[Key], classOf[Value], classOf[CusOutputFormat])</div></pre></td></tr></table></figure>
<h2 id="自定义数据源的改进"><a href="#自定义数据源的改进" class="headerlink" title="自定义数据源的改进"></a>自定义数据源的改进</h2><p>以上的写入方式依旧是不够优雅，那么如何才能实现</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">df.writer.format(&quot;xxxx&quot;).mode(&quot;xxx&quot;).save(&quot;path&quot;)</div></pre></td></tr></table></figure>
<p>跟进一下DataFrameWriter的<code>save</code>函数的代码发现其中创建了一个<code>ResolvedDataSource</code>实例，在创建的过程中会根据<code>format</code>制定的<code>provider</code>的信息反射得到相应的<code>Relation</code>，发现支持能通过由上面方式实现类型只能是<code>CreatableRelation</code>和<code>HadoopFsRelation</code>。</p>
<p>因此，修改自定义的<code>Relation</code>，使其继承<code>HadoopFsRelation</code>。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// TextRelation 构造参数可以随意</span></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">TextRelation</span>(<span class="params">override val paths: <span class="type">Array</span>[<span class="type">String</span>], parameters: <span class="type">Map</span>[<span class="type">String</span>, <span class="type">String</span>] = <span class="type">Map</span>.empty[<span class="type">String</span>, <span class="type">String</span>]</span>)(<span class="params">@transient val sqlContext: <span class="type">SQLContext</span></span>) <span class="keyword">extends</span> <span class="title">HadoopFsRelation</span> </span>&#123;</div><div class="line">  <span class="comment">// 不在需要重载schema，只需要重载dataSchema，在HadoopFsRelation内部会使用dataSchema来构造schema</span></div><div class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">dataSchema</span></span>: <span class="type">StructType</span> = &#123;</div><div class="line">    ...</div><div class="line">  &#125;</div><div class="line"> <span class="comment">// HadoopFsRelation 需要提供另一个Writer的工厂类</span></div><div class="line"> <span class="comment">// 这是和之前的例子有区别的地方，返回的OutputWriter用于数据的写入</span></div><div class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">prepareJobForWrite</span></span>(job: <span class="type">Job</span>): <span class="type">OutputWriterFactory</span> = &#123;</div><div class="line">    <span class="keyword">new</span> <span class="type">OutputWriterFactory</span> &#123;</div><div class="line">      <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">newInstance</span></span>(</div><div class="line">                                path: <span class="type">String</span>,</div><div class="line">                                dataSchema: <span class="type">StructType</span>,</div><div class="line">                                context: <span class="type">TaskAttemptContext</span>): <span class="type">OutputWriter</span> = &#123;</div><div class="line">        <span class="keyword">new</span> <span class="type">XXXOutputWriter</span>(path, dataSchema, context)</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line"><span class="comment">// 加载数据生成RDD[Row]，这里举了一个简单的例子</span></div><div class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">buildScan</span></span>(inputFiles: <span class="type">Array</span>[<span class="type">FileStatus</span>]): <span class="type">RDD</span>[<span class="type">Row</span>] = &#123;</div><div class="line">    <span class="keyword">val</span> job = <span class="keyword">new</span> <span class="type">Job</span>(sqlContext.sparkContext.hadoopConfiguration)</div><div class="line">    <span class="keyword">val</span> conf = <span class="type">SparkHadoopUtil</span>.get.getConfigurationFromJobContext(job)</div><div class="line"></div><div class="line">    <span class="keyword">if</span> (paths.nonEmpty) &#123;</div><div class="line">      <span class="comment">// 设置InputFormat路径</span></div><div class="line">      <span class="type">FileInputFormat</span>.setInputPaths(job, paths.map(<span class="keyword">new</span> <span class="type">Path</span>(_)): _*)</div><div class="line">    &#125;</div><div class="line">    <span class="keyword">val</span> rows = sqlContext.sparkContext.hadoopRDD(</div><div class="line">      conf.asInstanceOf[<span class="type">JobConf</span>], classOf[<span class="type">CusTextInputFormat</span>], classOf[<span class="type">LongWritable</span>], classOf[<span class="type">Text</span>])</div><div class="line">      <span class="comment">// 根据我们自定义的CusTextInputFormat，这个rows变量的类型是RDD[(LongWritable, Text)]</span></div><div class="line">      <span class="comment">// 我们需要转化为RDD[Row]的形式返回</span></div><div class="line">    rows.map[<span class="type">Row</span>] &#123; pair =&gt;</div><div class="line">      <span class="keyword">val</span> values = pair._2.toString.split(<span class="string">","</span>)</div><div class="line">      <span class="type">Row</span>(values: _*)</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>我们来看看<code>OutputWriter</code>应该如何实现，</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// 这部分代码自然是在excuter端运行的</span></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">TextOutputWriter</span>(<span class="params">path: <span class="type">String</span>, dataSchema: <span class="type">StructType</span>, context: <span class="type">TaskAttemptContext</span></span>) <span class="keyword">extends</span> <span class="title">OutputWriter</span> </span>&#123;</div><div class="line"></div><div class="line">  <span class="keyword">private</span> <span class="keyword">val</span> recordWriter: <span class="type">RecordWriter</span>[<span class="type">NullWritable</span>, <span class="type">Row</span>] = &#123;</div><div class="line">    <span class="comment">// 创建我们自定义的OutputFormat</span></div><div class="line">    <span class="keyword">new</span> <span class="type">CusTextOutputFormat</span>() &#123;</div><div class="line">    &#125;.getRecordWriter(context)</div><div class="line">  &#125;</div><div class="line">    </div><div class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">write</span></span>(row: <span class="type">Row</span>): <span class="type">Unit</span> = &#123;</div><div class="line">    recordWriter.write(<span class="type">NullWritable</span>.get(), row)</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">close</span></span>(): <span class="type">Unit</span> = &#123;</div><div class="line">    recordWriter.close(context)</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>最后提供一个<code>HadoopFsRelationProvider</code>就能使用我们自己定义的数据源了。</p>
<h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><ol>
<li>如何让<code>create table xxx stored as &#39;自定义的格式&#39; .....</code> 创建语句有效，还需要调研。</li>
<li>如何实现类似Parquet格式的数据写入，还需要研究。</li>
</ol>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol>
<li><a href="https://yq.aliyun.com/articles/8877" target="_blank" rel="external">Hadoop OutputFormat浅析</a></li>
<li><a href="http://my.oschina.net/leejun2005/blog/133424" target="_blank" rel="external">自定义 hadoop MapReduce InputFormat 切分输入文件</a></li>
<li><a href="https://github.com/databricks/spark-csv" target="_blank" rel="external">databricks/spark-csv</a></li>
<li><a href="https://github.com/JiananBear/Extra-DataSource-Demo/tree/master" target="_blank" rel="external">我自己写了一个小demo</a></li>
</ol>
]]></content>
    </entry>
    
  
  
</search>
